import org.apache.spark.sql.SparkSession
import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler, StandardScaler}
import org.apache.spark.ml.classification.RandomForestClassifier
import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

object AntiMoneyLaunderingModel {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("AntiMoneyLaunderingModel")
      .master("local[*]") // Use local mode for testing
      .getOrCreate()

    // -------------------------------
    // STEP 1: Load Dataset
    // -------------------------------
    val df = spark.read
      .option("header", "true")
      .option("inferSchema", "true")
      .csv("C:\\Users\\sgirishc\\Desktop\\fraud_detection\\project\\data\\big_dataset.csv")

    println(s"✅ Loaded dataset with ${df.count()} rows and ${df.columns.length} columns")

    // -------------------------------
    // STEP 2: Feature Engineering
    // -------------------------------
    val featureCols = df.columns.filter(_ != "Is_laundering")

    val assembler = new VectorAssembler()
      .setInputCols(featureCols)
      .setOutputCol("features_unscaled")

    val scaler = new StandardScaler()
      .setInputCol("features_unscaled")
      .setOutputCol("features")

    val labelIndexer = new StringIndexer()
      .setInputCol("Is_laundering")
      .setOutputCol("label")

    // -------------------------------
    // STEP 3: Define Model
    // -------------------------------
    val rf = new RandomForestClassifier()
      .setFeaturesCol("features")
      .setLabelCol("label")

    // -------------------------------
    // STEP 4: Build Pipeline
    // -------------------------------
    val pipeline = new Pipeline()
      .setStages(Array(assembler, scaler, labelIndexer, rf))

    // -------------------------------
    // STEP 5: Train/Test Split
    // -------------------------------
    val Array(trainData, testData) = df.randomSplit(Array(0.8, 0.2), seed = 42)

    // -------------------------------
    // STEP 6: Hyperparameter Tuning
    // -------------------------------
    val paramGrid = new ParamGridBuilder()
      .addGrid(rf.numTrees, Array(50, 100, 150))
      .addGrid(rf.maxDepth, Array(5, 10, 15))
      .addGrid(rf.featureSubsetStrategy, Array("auto", "sqrt", "log2"))
      .build()

    val evaluator = new BinaryClassificationEvaluator()
      .setLabelCol("label")
      .setMetricName("areaUnderROC")

    val cv = new CrossValidator()
      .setEstimator(pipeline)
      .setEvaluator(evaluator)
      .setEstimatorParamMaps(paramGrid)
      .setNumFolds(3)

    // -------------------------------
    // STEP 7: Train Tuned Model
    // -------------------------------
    val cvModel = cv.fit(trainData)

    // -------------------------------
    // STEP 8: Evaluate Tuned Model
    // -------------------------------
    val predictions = cvModel.transform(testData)
    val rocAuc = evaluator.evaluate(predictions)

    println(f"✅ Tuned Random Forest model trained. ROC-AUC on test set: $rocAuc%.4f")

    // -------------------------------
    // STEP 9: Save Tuned Model
    // -------------------------------
    cvModel.write.overwrite().save("models/tuned_rf_model")
    println("✅ Model saved to 'models/tuned_rf_model'")


    spark.stop()
  }
}
